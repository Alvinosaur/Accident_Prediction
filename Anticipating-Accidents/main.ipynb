{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Possible architecture changes:\n",
    "- process k frames at a time, not just 1 frame\n",
    "- bidirectional \n",
    "- apply MLP's to features, not just affine transformation\n",
    "- change to not have preset number of objects, handle any N objects\n",
    "- play with LSTM params, or variants (ex: GRU)\n",
    "\n",
    "Implementation Details:\n",
    "- objects can be diff size, so how to transform them into features? pass\n",
    "    entire image, but mask object so only its pixels are non-zero. this\n",
    "    way, same weights of same size can be applied to each object\n",
    "    - but in this case, weights depend on spatial location of an object,\n",
    "        so this is how transforming object into a feature encodes its\n",
    "        position, not just the object pixels, which are uninformative\n",
    "- paper uses hyperparam for number of objects in image. What if there\n",
    "    are less than num_objs in a given image? those extra remaining images\n",
    "    are completely blacked out, and after applying linear transform, we\n",
    "    just black out the entire feature so has no effect\n",
    "    - but if some objects are not visible and their pixels are all\n",
    "        blacked out, then why do we need to use the post-transformation\n",
    "        mask to zero everything out?\n",
    "- paper applies mask after softmax... doesn't this break the law of total\n",
    "  probability? Shouldn't we mask out nonexistent objects before the softmax?\n",
    "\n",
    "TODO:\n",
    "- look at what the input features X look like: just imshow them\n",
    "- finish loss function\n",
    "- onehot_to_binary?????\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Fattn(nn.Module):\n",
    "    def __init__(self, lstm_hidden_dim, obj_feat_dim):\n",
    "        \"\"\"Calculates alpha = softmax(f_attn(h_t-1, a_t)), which \n",
    "        assigns \n",
    "\n",
    "        Args:\n",
    "            lstm_hidden_dim ([type]): [description]\n",
    "            obj_feat_dim ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # linear transform of previous hidden state\n",
    "        self._lstm_hidden_dim = lstm_hidden_dim\n",
    "        self._obj_feat_dim = obj_feat_dim\n",
    "        self.hidden_linear = nn.Linear(\n",
    "            self._lstm_hidden_dim, self._obj_feat_dim, bias=False)\n",
    "        self.combined_linear = nn.Linear(\n",
    "            self._obj_feat_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, a, hprev, mask):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            a (Tensor): (B x K-1 x obj_feat_dim) feature vecs of diff objs\n",
    "            hprev (Tensor): (B x lstm_hidden_dim)\n",
    "            mask (Tensor): (B x K-1)\n",
    "        \"\"\"\n",
    "        # possibly perform some transform here to combine all hidden layer units\n",
    "        hprev = torch.squeeze(hprev)\n",
    "        # (B x K-1 x obj_feat_dim)\n",
    "        hprime = torch.unsqueeze(self.hidden_linear(hprev), 1)\n",
    "        e = torch.tanh(hprime + a)\n",
    "        # (B x K-1 x obj_feat_dim) -> (B x K-1)\n",
    "        alphas = self.combined_linear(e)\n",
    "        # calculate probability/importance of each K-1 object\n",
    "        # mask out any features that are non-existent\n",
    "        # alphas = torch.softmax(torch.multiply(alphas, mask), dim=-1)\n",
    "        alphas = torch.mul(torch.softmax(alphas, dim=-1), mask)\n",
    "        return alphas\n",
    "\n",
    "\n",
    "class AccidentDetection(nn.Module):\n",
    "    def __init__(self, img_dim, n_hidden_layers, img_feat_dim, obj_feat_dim, lstm_hidden_dim, lstm_dropout=0):\n",
    "        \"\"\"Main module encapsulating accident detection pipeline. Given a \n",
    "        video sequence of images, processes one frame at a time. \n",
    "\n",
    "        Args:\n",
    "            img_dim (int): size of flattened image\n",
    "            n_hidden_layers (int): number of hidden layers in LSTM\n",
    "            img_feat_dim (int): size of processed image feature\n",
    "            obj_feat_dim (int): size of processed object feature\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._img_dim = img_dim\n",
    "        self._obj_dim = img_dim  # entire image masked out except for object\n",
    "        self._n_hidden_layers = n_hidden_layers\n",
    "        self._img_feat_dim = img_feat_dim\n",
    "        self._obj_feat_dim = obj_feat_dim\n",
    "        self._lstm_hidden_dim = lstm_hidden_dim\n",
    "        self._num_dir = 1  # num directions, 2 if bidirectional\n",
    "        self._num_hidden_states = self._num_dir * self._n_hidden_layers\n",
    "\n",
    "        self._img_to_feat = nn.Linear(self._img_dim, self._img_feat_dim)\n",
    "        self._obj_to_feat = nn.Linear(self._obj_dim, self._obj_feat_dim)\n",
    "        self._obj_to_feat2 = nn.Linear(self._obj_feat_dim, self._img_feat_dim)\n",
    "        # probability of accident\n",
    "        self._out_to_pred = nn.Linear(self._lstm_hidden_dim, 1)\n",
    "        self._lstm = nn.LSTM(\n",
    "            input_size=self._img_feat_dim + self._obj_feat_dim,\n",
    "            hidden_size=self._lstm_hidden_dim,\n",
    "            num_layers=self._n_hidden_layers,\n",
    "            batch_first=False,\n",
    "            dropout=lstm_dropout)\n",
    "        self._fattn = Fattn(self._lstm_hidden_dim, self._img_feat_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): B x N x K x D\n",
    "                B = Batch size\n",
    "                N = num image frames per entry\n",
    "                K = 1 + num objects to focus on\n",
    "                D = input feature dimension (self._img_dim)\n",
    "                D_i = image feature dim\n",
    "                D_o = obj feature dim\n",
    "                D_o2 = 2nd obj feature dim\n",
    "        \"\"\"\n",
    "        B, N, K, D = x.shape\n",
    "        # all zeros for an obj index in a frame of a specific batch if that obj\n",
    "        # isn't present\n",
    "        # no mask for first of K since that represents entire image, not an obj\n",
    "        # (B x N x K-1 x 1)\n",
    "        obj_mask = torch.sum(x[:, :, 1:], dim=-1, keepdim=True)\n",
    "\n",
    "        # transform full image input vec into img feature\n",
    "        # (B x N x D_i)\n",
    "        img_feat = self._img_to_feat(x[:, :, 0, :])\n",
    "\n",
    "        # transform each obj input vec into obj feature\n",
    "        # (B x N x K-1 x D_o)\n",
    "        obj_feat = self._obj_to_feat(x[:, :, 1:, :])\n",
    "\n",
    "        # mask out any obj features where obj isn't present\n",
    "        # (B x N x K-1, D_o) = (B x N x K-1 x D_o) * (B x N x K-1 x 1) < brdcst\n",
    "        obj_feat = torch.mul(obj_feat, obj_mask)\n",
    "        # 2nd affine transform\n",
    "        # (B x N x K-1, D_o) -> (B x N x K-1, D_i)\n",
    "        obj_feat = self._obj_to_feat2(obj_feat)\n",
    "\n",
    "        # intialize LSTM hidden state and\n",
    "        hidden_state = torch.zeros(\n",
    "            (self._num_hidden_states, B, self._lstm_hidden_dim))\n",
    "        cell_state = torch.zeros(\n",
    "            (self._num_hidden_states, B, self._lstm_hidden_dim))\n",
    "        prev_output = torch.zeros((1, B, self._lstm_hidden_dim))\n",
    "\n",
    "        # track all info\n",
    "        all_alphas = []\n",
    "        all_predictions = []\n",
    "\n",
    "        for fi in range(N):\n",
    "            # (B x K-1 x D_i)\n",
    "            cur_obj_feat = obj_feat[:, fi, :, :]\n",
    "            # (B x D_i)\n",
    "            cur_img_feat = img_feat[:, fi, :]\n",
    "            # (B x K-1 x 1)\n",
    "            cur_obj_mask = obj_mask[:, fi, :]\n",
    "\n",
    "            # (B x K-1)\n",
    "            alphas = self._fattn(cur_obj_feat, prev_output, cur_obj_mask)\n",
    "            # weighted each object feature by its attention alphas\n",
    "            # (B x K-1 x D_i)\n",
    "            w_obj_feat = torch.mul(cur_obj_feat, alphas)\n",
    "            # sum up all K-1 features to produce weighted sum\n",
    "            # (B x D_i)\n",
    "            w_obj_feat = torch.sum(w_obj_feat, dim=1)\n",
    "\n",
    "            # (B x 2*D_i)\n",
    "            fusion = torch.cat([cur_img_feat, w_obj_feat], dim=1)\n",
    "            # (1 x B x 2*D_i) since lstm takes (seq_len, batch, input_size)\n",
    "            fusion = torch.unsqueeze(fusion, dim=0)\n",
    "            prev_output, (hidden_state, cell_state) = self._lstm(\n",
    "                fusion, (hidden_state, cell_state))\n",
    "\n",
    "            # possible combine sequence of outputs\n",
    "            prev_output = torch.squeeze(prev_output, dim=0)\n",
    "\n",
    "            # (B x H)\n",
    "            logits = self._out_to_pred(prev_output)\n",
    "            # predictions = torch.softmax(logits, dim=1)\n",
    "            predictions = logits  # just use logits directly\n",
    "\n",
    "            # save all outputs\n",
    "            all_alphas.append(alphas)\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions, dim=1)\n",
    "        return all_alphas, all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "def onehot_to_binary(batch_ys: np.ndarray):\n",
    "    # (2 x 1)\n",
    "    has_accident = np.array([[0], [1]])\n",
    "    # (B x 2)(2 x 1) = (B x 1)\n",
    "    return batch_ys @ has_accident\n",
    "\n",
    "\n",
    "def train_epoch(model, loss_fn, files, batch_indices,\n",
    "                optimizer, device):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for batch_i in batch_indices:\n",
    "        batch_data = np.load(files[batch_i])\n",
    "        batch_xs = torch.Tensor(batch_data['data']).to(device)\n",
    "\n",
    "        n_frames = batch_xs.shape[1]\n",
    "        batch_ys = onehot_to_binary(batch_data['labels'])\n",
    "        batch_ys = np.tile(batch_ys, (1, n_frames))\n",
    "        batch_ys = torch.Tensor(batch_ys).to(device)\n",
    "\n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "\n",
    "        alphas, logits = model(batch_xs)\n",
    "        loss = loss_fn(logits, batch_ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "    avg_loss /= len(files)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def eval_model(model, loss_fn, files, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for fname in files:\n",
    "            batch_data = np.load(fname)\n",
    "            batch_xs = torch.Tensor(batch_data['data']).to(device)\n",
    "            batch_ys = torch.Tensor(batch_data['labels']).to(device)\n",
    "\n",
    "            alphas, logits = model(batch_xs)\n",
    "            loss = loss_fn(logits, batch_ys).detach()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "    avg_loss /= len(files)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, loss_fn, progress_dir,\n",
    "                train_files, eval_files, num_epochs, device):\n",
    "    # detect anomalies in calculating gradient\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    if not os.path.exists(progress_dir):\n",
    "        os.mkdir(progress_dir)\n",
    "\n",
    "    num_batches = len(train_files)\n",
    "    batch_indices = np.arange(num_batches)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tstart = time.time()\n",
    "        print(\"\\n========== Epoch {} ==========\".format(epoch))\n",
    "\n",
    "        # shuffle train dataset\n",
    "        np.random.shuffle(batch_indices)\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, loss_fn, train_files, batch_indices,\n",
    "                                 optimizer, device)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = eval_model(model, loss_fn, eval_files, device)\n",
    "\n",
    "        # decrease learning rate with scheduler\n",
    "        scheduler.step(metrics=val_loss)\n",
    "\n",
    "        # save model weights for this epoch\n",
    "        unique_name = \"epoch_%d.h5\" % (epoch)\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss}\n",
    "        torch.save(checkpoint, os.path.join(progress_dir, unique_name))\n",
    "\n",
    "        tend = time.time()\n",
    "        print(\"Epoch %d Elapsed Time: %.2fs\" % (epoch, tend - tstart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    # Command-line flags are defined here.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--print_to_file', dest='print_to_file', type=bool,\n",
    "                        default=True, help=\"Whether to save printout to file\")\n",
    "    parser.add_argument('--num_epochs', dest='num_epochs', type=int,\n",
    "                        default=10, help=\"Number of epochs to train on.\")\n",
    "    parser.add_argument('--lr', dest='lr', type=float,\n",
    "                        default=3e-4, help=\"The learning rate.\")\n",
    "    parser.add_argument('--model_path', dest='model_path', type=str,\n",
    "                        default=\"\", help=\"Optionally load an existing model\")\n",
    "    parser.add_argument('--train', dest='train', action='store_true',\n",
    "                        help=\"Whether to train or run demo\")\n",
    "    parser.set_defaults(train=True)\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "from accident_detection import AccidentDetection\n",
    "import model_utils\n",
    "import utils\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = utils.parse_arguments()\n",
    "    with open(\"params.yaml\") as f:\n",
    "        params = yaml.load(f)\n",
    "\n",
    "    timestamp = int(time.time())\n",
    "    progress_dir = os.path.join(\n",
    "        params[\"root\"], params[\"save_path\"], \"%d\" % timestamp)\n",
    "    if not os.path.exists(progress_dir):\n",
    "        os.mkdir(progress_dir)\n",
    "    if args.print_to_file:\n",
    "        sys.stdout = open(os.path.join(progress_dir, \"output.txt\"), \"w\")\n",
    "    print(\"Params: %s\" % str(params))\n",
    "    print(\"Args: %s\" % str(args))\n",
    "\n",
    "    train_dir = os.path.join(params[\"root\"], params[\"train_path\"])\n",
    "    train_files = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\n",
    "    eval_dir = os.path.join(params[\"root\"], params[\"test_path\"])\n",
    "    eval_files = [os.path.join(eval_dir, f) for f in os.listdir(eval_dir)]\n",
    "    video_dir = os.path.join(params[\"root\"], params[\"video_path\"])\n",
    "    video_files = [os.path.join(video_dir, f) for f in os.listdir(video_dir)]\n",
    "\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    num_workers = 8 if cuda else 0\n",
    "    if cuda:\n",
    "        print(\"CUDA GPU!\")\n",
    "    else:\n",
    "        print(\"CPU!\")\n",
    "\n",
    "    # define model, optimizer, lr scheduler\n",
    "    model = AccidentDetection(\n",
    "        img_dim=params[\"img_dim\"], n_hidden_layers=params[\"n_hidden_layers\"],\n",
    "        img_feat_dim=params[\"img_feat_dim\"],\n",
    "        obj_feat_dim=params[\"obj_feat_dim\"],\n",
    "        lstm_hidden_dim=params[\"hidden_feat_dim\"],\n",
    "        lstm_dropout=params[\"lstm_dropout\"])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # optionally load from some previous checkpoint\n",
    "    if args.model_path != \"\":\n",
    "        print(\"Loading Existing Model: %s\" % args.model_path)\n",
    "        checkpoint = torch.load(args.model_path)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "    else:\n",
    "        model.apply(model_utils.init_weights)\n",
    "\n",
    "    if args.train:\n",
    "        model_utils.train_model(model, optimizer, scheduler, loss_fn,\n",
    "                                progress_dir, train_files, eval_files,\n",
    "                                args.num_epochs, device)\n",
    "    else:\n",
    "        # run demo\n",
    "        pass\n"
   ]
  }
 ]
}