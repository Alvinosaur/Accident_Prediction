{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "re-7C71wGIt1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Possible architecture changes:\n",
        "- process k frames at a time, not just 1 frame\n",
        "- bidirectional\n",
        "- apply MLP's to features, not just affine transformation\n",
        "- change to not have preset number of objects, handle any N objects\n",
        "- play with LSTM params, or variants (ex: GRU)\n",
        "\n",
        "Implementation Details:\n",
        "- objects can be diff size, so how to transform them into features? pass\n",
        "    entire image, but mask object so only its pixels are non-zero. this\n",
        "    way, same weights of same size can be applied to each object\n",
        "    - but in this case, weights depend on spatial location of an object,\n",
        "        so this is how transforming object into a feature encodes its\n",
        "        position, not just the object pixels, which are uninformative\n",
        "- paper uses hyperparam for number of objects in image. What if there\n",
        "    are less than num_objs in a given image? those extra remaining images\n",
        "    are completely blacked out, and after applying linear transform, we\n",
        "    just black out the entire feature so has no effect\n",
        "    - but if some objects are not visible and their pixels are all\n",
        "        blacked out, then why do we need to use the post-transformation\n",
        "        mask to zero everything out?\n",
        "- paper applies mask after softmax... doesn't this break the law of total\n",
        "  probability? Shouldn't we mask out nonexistent objects before the softmax?\n",
        "\n",
        "Possible Bugs:\n",
        "- try with no frame weights\n",
        "- try alphas = torch.softmax(torch.multiply(alphas, mask), dim=1) line 104\n",
        "- prev_output, (hidden_state, cell_state) = self._lstm(\n",
        "                fusion, (hidden_state, cell_state))\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class AccidentLoss(object):\n",
        "    def __init__(self, n_frames, device):\n",
        "        self.n_frames = n_frames\n",
        "        pos_weights = torch.exp(\n",
        "            - torch.arange(self.n_frames - 1, -1, -1) / 20.0).view(-1, 1)\n",
        "        neg_weights = torch.ones((n_frames, 1))\n",
        "        # (n_frames x 2)\n",
        "        self.frame_weights = torch.cat([neg_weights, pos_weights], dim=1)\n",
        "        self.frame_weights = self.frame_weights.to(device)\n",
        "        self.frame_weights.requires_grad = False\n",
        "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
        "        self.nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "    def __call__(self, logits, labels):\n",
        "        # (n_frames x B x 2)\n",
        "        loss = self.log_softmax(logits)\n",
        "        # (n_frames x B x 2) multiply each frame's outputs with specific weight\n",
        "        loss = torch.mul(self.frame_weights, loss)\n",
        "        # (n_frames*B x 2) following NLLLoss's expected input of (minibatch, C)\n",
        "        loss = loss.view(-1, 2)\n",
        "        labels = labels.view(-1)\n",
        "        # compute average loss over all frames of entire batch\n",
        "        loss = self.nll_loss(loss, labels)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class Fattn(nn.Module):\n",
        "    def __init__(self, lstm_hidden_dim, obj_feat_dim):\n",
        "        \"\"\"Calculates alpha = softmax(f_attn(h_t-1, a_t)), which\n",
        "        assigns\n",
        "\n",
        "        Args:\n",
        "            lstm_hidden_dim ([type]): [description]\n",
        "            obj_feat_dim ([type]): [description]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # linear transform of previous hidden state\n",
        "        self._lstm_hidden_dim = lstm_hidden_dim\n",
        "        self._obj_feat_dim = obj_feat_dim\n",
        "        self.hidden_linear = nn.Linear(\n",
        "            self._lstm_hidden_dim, self._obj_feat_dim, bias=False)\n",
        "        self.combined_linear = nn.Linear(\n",
        "            self._obj_feat_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, a, hprev, mask):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Args:\n",
        "            a (Tensor): (B x K-1 x obj_feat_dim) feature vecs of diff objs\n",
        "            hprev (Tensor): (B x lstm_hidden_dim)\n",
        "            mask (Tensor): (B x K-1)\n",
        "        \"\"\"\n",
        "        # possibly perform some transform here to combine all hidden layer units\n",
        "        hprev = torch.squeeze(hprev)\n",
        "        # (B x 1 x obj_feat_dim)\n",
        "        hprime = torch.unsqueeze(self.hidden_linear(hprev), 1)\n",
        "        e = torch.tanh(hprime + a)\n",
        "        # (B x K-1 x obj_feat_dim) -> (B x K-1 x 1)\n",
        "        alphas = self.combined_linear(e)\n",
        "        # calculate probability/importance of each K-1 object\n",
        "        # mask out any features that are non-existent\n",
        "        alphas = torch.softmax(torch.mul(alphas, mask), dim=1)\n",
        "        # alphas = torch.mul(torch.softmax(alphas, dim=1), mask)\n",
        "        # assert(torch.sum(alphas, axis=1) == ones)\n",
        "        # probability of each obj feature should sum to 1 for a batch\n",
        "        # but after post-multiplying by mask, this isn't true anymore\n",
        "        return alphas\n",
        "\n",
        "\n",
        "class AccidentDetection(nn.Module):\n",
        "    def __init__(self, img_dim, n_hidden_layers, img_feat_dim, obj_feat_dim, lstm_hidden_dim, device, lstm_dropout=0):\n",
        "        \"\"\"Main module encapsulating accident detection pipeline. Given a\n",
        "        video sequence of images, processes one frame at a time. Output for a\n",
        "        given frame is a (1 x 2) [1-p(accident), p(accident)].\n",
        "\n",
        "        Args:\n",
        "            img_dim (int): size of flattened image\n",
        "            n_hidden_layers (int): number of hidden layers in LSTM\n",
        "            img_feat_dim (int): size of processed image feature\n",
        "            obj_feat_dim (int): size of processed object feature\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._img_dim = img_dim\n",
        "        self._obj_dim = img_dim  # entire image masked out except for object\n",
        "        self._n_hidden_layers = n_hidden_layers\n",
        "        self._img_feat_dim = img_feat_dim\n",
        "        self._obj_feat_dim = obj_feat_dim\n",
        "        self._lstm_hidden_dim = lstm_hidden_dim\n",
        "        self._num_dir = 1  # num directions, 2 if bidirectional\n",
        "        self._num_hidden_states = self._num_dir * self._n_hidden_layers\n",
        "        self.device = device\n",
        "\n",
        "        self._img_to_feat = nn.Linear(self._img_dim, self._img_feat_dim)\n",
        "        self._obj_to_feat = nn.Linear(self._obj_dim, self._obj_feat_dim)\n",
        "        self._obj_to_feat2 = nn.Linear(self._obj_feat_dim, self._img_feat_dim)\n",
        "        # output = [1-prob(accident), prob(accident)]\n",
        "        self._out_to_pred = nn.Linear(self._lstm_hidden_dim, 2)\n",
        "        self._lstm = nn.LSTM(\n",
        "            input_size=self._img_feat_dim + self._obj_feat_dim,\n",
        "            hidden_size=self._lstm_hidden_dim,\n",
        "            num_layers=self._n_hidden_layers,\n",
        "            batch_first=False,\n",
        "            dropout=lstm_dropout)\n",
        "        self._fattn = Fattn(self._lstm_hidden_dim, self._img_feat_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): B x N x K x D\n",
        "                B = Batch size\n",
        "                N = num image frames per entry\n",
        "                K = 1 + num objects to focus on\n",
        "                D = input feature dimension (self._img_dim)\n",
        "                D_i = image feature dim\n",
        "                D_o = obj feature dim\n",
        "                D_o2 = 2nd obj feature dim\n",
        "        \"\"\"\n",
        "        B, N, K, D = x.shape\n",
        "        # all zeros for an obj index in a frame of a specific batch if that obj\n",
        "        # isn't present\n",
        "        # no mask for first of K since that represents entire image, not an obj\n",
        "        # (B x N x K-1 x 1)\n",
        "        zeros = torch.zeros((B, N, K - 1, 1)).to(self.device)\n",
        "        obj_mask = torch.sum(x[:, :, 1:], dim=-1, keepdim=True)\n",
        "        obj_mask = torch.isclose(obj_mask, zeros, atol=1e-06)\n",
        "        obj_mask = (obj_mask != True).float()\n",
        "\n",
        "        # transform full image input vec into img feature\n",
        "        # (B x N x D_i)\n",
        "        img_feat = self._img_to_feat(x[:, :, 0, :])\n",
        "\n",
        "        # transform each obj input vec into obj feature\n",
        "        # (B x N x K-1 x D_o)\n",
        "        obj_feat = self._obj_to_feat(x[:, :, 1:, :])\n",
        "\n",
        "        # mask out any obj features where obj isn't present\n",
        "        # (B x N x K-1, D_o) = (B x N x K-1 x D_o) * (B x N x K-1 x 1) < brdcst\n",
        "        obj_feat = torch.mul(obj_feat, obj_mask)\n",
        "        # 2nd affine transform\n",
        "        # (B x N x K-1, D_o) -> (B x N x K-1, D_i)\n",
        "        obj_feat = self._obj_to_feat2(obj_feat)\n",
        "\n",
        "        # intialize LSTM hidden state and\n",
        "        hidden_state = torch.zeros(\n",
        "            (self._num_hidden_states, B, self._lstm_hidden_dim)).to(self.device)\n",
        "        cell_state = torch.zeros(\n",
        "            (self._num_hidden_states, B, self._lstm_hidden_dim)).to(self.device)\n",
        "        prev_output = torch.zeros(\n",
        "            (1, B, self._lstm_hidden_dim)).to(self.device)\n",
        "\n",
        "        # track all info\n",
        "        all_alphas = []\n",
        "        all_predictions = []\n",
        "\n",
        "        for fi in range(N):\n",
        "            # (B x K-1 x D_i)\n",
        "            cur_obj_feat = obj_feat[:, fi, :, :]\n",
        "            # (B x D_i)\n",
        "            cur_img_feat = img_feat[:, fi, :]\n",
        "            # (B x K-1 x 1)\n",
        "            cur_obj_mask = obj_mask[:, fi, :]\n",
        "\n",
        "            # (B x K-1)\n",
        "            alphas = self._fattn(cur_obj_feat, prev_output, cur_obj_mask)\n",
        "            # weighted each object feature by its attention alphas\n",
        "            # (B x K-1 x D_i)\n",
        "            w_obj_feat = torch.mul(cur_obj_feat, alphas)\n",
        "            # sum up all K-1 features to produce weighted sum\n",
        "            # (B x D_i)\n",
        "            w_obj_feat = torch.sum(w_obj_feat, dim=1)\n",
        "\n",
        "            # (B x 2*D_i)\n",
        "            fusion = torch.cat([cur_img_feat, w_obj_feat], dim=1)\n",
        "            # (1 x B x 2*D_i) since lstm takes (seq_len, batch, input_size)\n",
        "            fusion = torch.unsqueeze(fusion, dim=0)\n",
        "            prev_output, (hidden_state, cell_state) = self._lstm(\n",
        "                fusion, (hidden_state, cell_state))\n",
        "\n",
        "            # possible combine sequence of outputs\n",
        "            prev_output = torch.squeeze(prev_output, dim=0)\n",
        "\n",
        "            # (B x H)\n",
        "            logits = self._out_to_pred(prev_output)\n",
        "            # predictions = torch.softmax(logits, dim=1)\n",
        "            predictions = logits  # CrossEntropyLoss already applies LogSoftmax\n",
        "\n",
        "            # save all outputs\n",
        "            all_alphas.append(alphas)\n",
        "            all_predictions.append(predictions)\n",
        "\n",
        "        # (N x B x 2) --> (B x N x 2)\n",
        "        all_predictions = torch.stack(all_predictions).transpose(1, 0)\n",
        "        return all_alphas, all_predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxKa4D4-GOnQ"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "def train_epoch(model, loss_fn, files, batch_indices,\n",
        "                optimizer, device):\n",
        "    model.train()\n",
        "    avg_loss = 0.0\n",
        "\n",
        "    for batch_i in batch_indices:\n",
        "        batch_data = np.load(files[batch_i])\n",
        "        batch_xs = torch.Tensor(batch_data['data']).to(device)\n",
        "\n",
        "        batch_size, n_frames = batch_xs.shape[0:2]\n",
        "        # accident: [0, 1]  -->  class = 1\n",
        "        # no accident: [1, 0]  --> class = 0\n",
        "        # model output = [1-p(accident), p(accident)]\n",
        "        batch_ys = torch.Tensor(batch_data['labels'][:, 1]).long().to(device)\n",
        "        batch_ys = batch_ys.unsqueeze(0)\n",
        "        # (N x B x 1)\n",
        "        batch_ys = batch_ys.repeat(n_frames, 1, 1)\n",
        "\n",
        "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
        "\n",
        "        alphas, predictions = model(batch_xs)\n",
        "        loss = loss_fn(predictions, batch_ys)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "    avg_loss /= len(files)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def eval_model(model, loss_fn, files, device):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        avg_loss = 0.0\n",
        "\n",
        "        for fname in files:\n",
        "            batch_data = np.load(fname)\n",
        "            batch_xs = torch.Tensor(batch_data['data']).to(device)\n",
        "\n",
        "            batch_size, n_frames = batch_xs.shape[0:2]\n",
        "            # accident: [0, 1]  -->  class = 1\n",
        "            # no accident: [1, 0]  --> class = 0\n",
        "            # model output = [1-p(accident), p(accident)]\n",
        "            batch_ys = torch.Tensor(batch_data['labels'][:, 1]).long().to(device)\n",
        "            batch_ys = batch_ys.unsqueeze(0)\n",
        "            # (N x B x 1)\n",
        "            batch_ys = batch_ys.repeat(n_frames, 1, 1)\n",
        "\n",
        "            alphas, predictions = model(batch_xs)\n",
        "            loss = loss_fn(predictions, batch_ys).detach()\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "    avg_loss /= len(files)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, scheduler, loss_fn, progress_dir,\n",
        "                train_files, eval_files, num_epochs, device):\n",
        "    # detect anomalies in calculating gradient\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    if not os.path.exists(progress_dir):\n",
        "        os.mkdir(progress_dir)\n",
        "\n",
        "    num_batches = len(train_files)\n",
        "    batch_indices = np.arange(num_batches)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        tstart = time.time()\n",
        "        print(\"\\n========== Epoch {} ==========\".format(epoch))\n",
        "\n",
        "        # shuffle train dataset\n",
        "        np.random.shuffle(batch_indices)\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, loss_fn, train_files, batch_indices,\n",
        "                                 optimizer, device)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss = eval_model(model, loss_fn, eval_files, device)\n",
        "\n",
        "        # decrease learning rate with scheduler\n",
        "        scheduler.step(metrics=val_loss)\n",
        "\n",
        "        # save model weights for this epoch\n",
        "        unique_name = \"epoch_%d.h5\" % (epoch)\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss}\n",
        "        torch.save(checkpoint, os.path.join(progress_dir, unique_name))\n",
        "\n",
        "        tend = time.time()\n",
        "        print(\"Epoch: %d, Train Loss: %.3f, Val loss: %.3f, Elapsed Time: %.2fs\" % (\n",
        "            epoch, float(train_loss), float(val_loss), tend - tstart),\n",
        "            flush=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPyiCfpFGRpa"
      },
      "source": [
        "# import argparse\n",
        "\n",
        "\n",
        "# def parse_arguments():\n",
        "#     # Command-line flags are defined here.\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--print_to_file', dest='print_to_file', type=bool,\n",
        "#                         default=True, help=\"Whether to save printout to file\")\n",
        "#     parser.add_argument('--num_epochs', dest='num_epochs', type=int,\n",
        "#                         default=10, help=\"Number of epochs to train on.\")\n",
        "#     parser.add_argument('--lr', dest='lr', type=float,\n",
        "#                         default=3e-4, help=\"The learning rate.\")\n",
        "#     parser.add_argument('--model_path', dest='model_path', type=str,\n",
        "#                         default=\"\", help=\"Optionally load an existing model\")\n",
        "#     parser.add_argument('--train', dest='train', action='store_true',\n",
        "#                         help=\"Whether to train or run demo\")\n",
        "#     parser.set_defaults(train=True)\n",
        "#     return parser.parse_args()\n",
        "\n",
        "# Notebook version\n",
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.print_to_file = False\n",
        "        self.num_epochs = 20\n",
        "        self.lr = 1e-3\n",
        "        self.model_path = \"\"\n",
        "        self.train = True\n",
        "    def __str__(self):\n",
        "        output = \"print_to_file: {}, num_epochs: {}, lr: {}, model_path: {}, train: {}\".format(self.print_to_file, self.num_epochs, self.lr, self.model_path, self.train)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQRffEx1GTX2"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "\n",
        "# from accident_detection import AccidentDetection\n",
        "# import model_utils\n",
        "# import utils\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # args = utils.parse_arguments()\n",
        "    args = Arguments()\n",
        "    with open(\"/content/drive/My Drive/School-Related/F20/Pattern Recognition Theory/Project/params.yaml\") as f:\n",
        "        params = yaml.load(f)\n",
        "\n",
        "    timestamp = int(time.time())\n",
        "    progress_dir = os.path.join(\n",
        "        params[\"root\"], params[\"save_path\"], \"%d\" % timestamp)\n",
        "    if not os.path.exists(progress_dir):\n",
        "        os.mkdir(progress_dir)\n",
        "    print(args, flush=True)\n",
        "    if args.print_to_file:\n",
        "        sys.stdout = open(os.path.join(progress_dir, \"output.txt\"), \"w\")\n",
        "    print(\"Params: %s\" % params, flush=True)\n",
        "    print(\"Args: %s\" % args, flush=True)\n",
        "\n",
        "    train_dir = os.path.join(params[\"root\"], params[\"train_path\"])\n",
        "    train_files = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\n",
        "    eval_dir = os.path.join(params[\"root\"], params[\"test_path\"])\n",
        "    eval_files = [os.path.join(eval_dir, f) for f in os.listdir(eval_dir)]\n",
        "    video_dir = os.path.join(params[\"root\"], params[\"video_path\"])\n",
        "    video_files = [os.path.join(video_dir, f) for f in os.listdir(video_dir)]\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "    num_workers = 8 if cuda else 0\n",
        "    if cuda:\n",
        "        print(\"CUDA GPU!\")\n",
        "    else:\n",
        "        print(\"CPU!\")\n",
        "\n",
        "    # define model, optimizer, lr scheduler\n",
        "    model = AccidentDetection(\n",
        "        img_dim=params[\"img_dim\"], n_hidden_layers=params[\"n_hidden_layers\"],\n",
        "        img_feat_dim=params[\"img_feat_dim\"],\n",
        "        obj_feat_dim=params[\"obj_feat_dim\"],\n",
        "        lstm_hidden_dim=params[\"hidden_feat_dim\"],\n",
        "        lstm_dropout=params[\"lstm_dropout\"],\n",
        "        device=device)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
        "\n",
        "    n_frames = params[\"n_frames\"]\n",
        "    loss_fn = AccidentLoss(n_frames, device)\n",
        "\n",
        "    # optionally load from some previous checkpoint\n",
        "    if args.model_path != \"\":\n",
        "        print(\"Loading Existing Model: %s\" % args.model_path)\n",
        "        checkpoint = torch.load(args.model_path)\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "    else:\n",
        "        model.apply(init_weights)\n",
        "\n",
        "    if args.train:\n",
        "        train_model(model, optimizer, scheduler, loss_fn,\n",
        "                                progress_dir, train_files[:15], eval_files,\n",
        "                                args.num_epochs, device)\n",
        "    else:\n",
        "        # run demo\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}